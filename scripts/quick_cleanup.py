#!/usr/bin/env python3
"""
å¿«é€Ÿæ¸…ç†è„šæœ¬ - æ¸…ç†multiprocessingå­¤å„¿è¿›ç¨‹å’ŒGPUå†…å­˜
"""

import logging
import sys
import time

try:
    import torch
except ImportError:
    torch = None

try:
    import psutil
except ImportError:
    psutil = None
    print("è­¦å‘Š: psutilæœªå®‰è£…ï¼Œæ— æ³•æ¸…ç†è¿›ç¨‹")
    sys.exit(1)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def cleanup_multiprocessing_processes():
    """æ¸…ç†multiprocessingå­¤å„¿è¿›ç¨‹"""
    if not psutil:
        return 0

    cleaned_count = 0
    current_pid = psutil.Process().pid

    logger.info(f"å½“å‰è¿›ç¨‹PID: {current_pid}")

    # æŸ¥æ‰¾æ‰€æœ‰multiprocessingç›¸å…³è¿›ç¨‹
    for proc in psutil.process_iter(['pid', 'name', 'cmdline', 'ppid', 'status']):
        try:
            cmdline = proc.info.get('cmdline', [])
            if cmdline and any('multiprocessing' in str(arg) for arg in cmdline):
                pid = proc.info['pid']
                ppid = proc.info['ppid']
                status = proc.info['status']

                # æ£€æŸ¥æ˜¯å¦æ˜¯å­¤å„¿è¿›ç¨‹ï¼ˆçˆ¶è¿›ç¨‹ä¸å­˜åœ¨æˆ–ä¸º1ï¼‰
                is_orphaned = False
                try:
                    if ppid == 1:
                        is_orphaned = True
                        reason = "çˆ¶è¿›ç¨‹ä¸ºinit"
                    else:
                        parent = psutil.Process(ppid)
                        if not parent.is_running():
                            is_orphaned = True
                            reason = "çˆ¶è¿›ç¨‹å·²æ­»"
                except psutil.NoSuchProcess:
                    is_orphaned = True
                    reason = "çˆ¶è¿›ç¨‹ä¸å­˜åœ¨"

                if is_orphaned or True:  # æš‚æ—¶æ¸…ç†æ‰€æœ‰multiprocessingè¿›ç¨‹
                    memory_info = proc.memory_info() if hasattr(proc, 'memory_info') else None
                    memory_mb = memory_info.rss / 1024 / 1024 if memory_info else 0

                    logger.info(f"å‘ç°multiprocessingè¿›ç¨‹: PID {pid}, PPID {ppid}, "
                              f"çŠ¶æ€: {status}, å†…å­˜: {memory_mb:.1f}MB, å­¤å„¿: {is_orphaned}")

                    try:
                        proc.terminate()
                        proc.wait(timeout=3)
                        cleaned_count += 1
                        logger.info(f"âœ“ è¿›ç¨‹ {pid} å·²ä¼˜é›…ç»ˆæ­¢")
                    except psutil.TimeoutExpired:
                        proc.kill()
                        cleaned_count += 1
                        logger.warning(f"âœ“ è¿›ç¨‹ {pid} å·²å¼ºåˆ¶ç»ˆæ­¢")
                    except (psutil.NoSuchProcess, psutil.AccessDenied) as e:
                        logger.warning(f"âœ— æ— æ³•å¤„ç†è¿›ç¨‹ {pid}: {e}")

        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue

    return cleaned_count

def cleanup_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    if not torch or not torch.cuda.is_available():
        logger.info("CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUå†…å­˜æ¸…ç†")
        return

    try:
        # è·å–æ¸…ç†å‰çŠ¶æ€
        before_allocated = torch.cuda.memory_allocated()
        before_reserved = torch.cuda.memory_reserved()

        logger.info(f"æ¸…ç†å‰GPUå†…å­˜: å·²åˆ†é… {before_allocated / 1024**3:.2f}GB, "
                   f"å·²ä¿ç•™ {before_reserved / 1024**3:.2f}GB")

        # å¤šè½®æ¸…ç†
        import gc
        for i in range(5):
            gc.collect()
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            time.sleep(0.1)

        # è·å–æ¸…ç†åçŠ¶æ€
        after_allocated = torch.cuda.memory_allocated()
        after_reserved = torch.cuda.memory_reserved()

        freed_allocated = (before_allocated - after_allocated) / 1024**3
        freed_reserved = (before_reserved - after_reserved) / 1024**3

        logger.info(f"æ¸…ç†åGPUå†…å­˜: å·²åˆ†é… {after_allocated / 1024**3:.2f}GB, "
                   f"å·²ä¿ç•™ {after_reserved / 1024**3:.2f}GB")
        logger.info(f"é‡Šæ”¾å†…å­˜: å·²åˆ†é… {freed_allocated:.2f}GB, "
                   f"å·²ä¿ç•™ {freed_reserved:.2f}GB")

    except Exception as e:
        logger.error(f"GPUå†…å­˜æ¸…ç†å¤±è´¥: {e}")

def main():
    logger.info("ğŸ§¹ å¼€å§‹å¿«é€Ÿæ¸…ç†...")

    # æ¸…ç†multiprocessingè¿›ç¨‹
    logger.info("ğŸ“‹ æŸ¥æ‰¾å¹¶æ¸…ç†multiprocessingå­¤å„¿è¿›ç¨‹...")
    cleaned_processes = cleanup_multiprocessing_processes()
    logger.info(f"âœ… æ¸…ç†äº† {cleaned_processes} ä¸ªmultiprocessingè¿›ç¨‹")

    # æ¸…ç†GPUå†…å­˜
    logger.info("ğŸ® æ¸…ç†GPUå†…å­˜...")
    cleanup_gpu_memory()

    # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
    logger.info("ğŸ“Š æœ€ç»ˆèµ„æºçŠ¶æ€:")
    if torch and torch.cuda.is_available():
        final_allocated = torch.cuda.memory_allocated() / 1024**3
        final_reserved = torch.cuda.memory_reserved() / 1024**3
        logger.info(f"   GPUå†…å­˜: å·²åˆ†é… {final_allocated:.2f}GB, å·²ä¿ç•™ {final_reserved:.2f}GB")

    memory = psutil.virtual_memory()
    logger.info(f"   ç³»ç»Ÿå†…å­˜: {memory.percent:.1f}% ä½¿ç”¨ ({memory.used / 1024**3:.1f}GB / {memory.total / 1024**3:.1f}GB)")

    logger.info("ğŸ‰ å¿«é€Ÿæ¸…ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()