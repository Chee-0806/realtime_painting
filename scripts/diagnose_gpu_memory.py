#!/usr/bin/env python3
"""
GPUå†…å­˜è¯Šæ–­è„šæœ¬

ç”¨äºè¯Šæ–­GPUå†…å­˜å ç”¨çš„åŸå› ï¼Œç‰¹åˆ«æ˜¯PyTorchæ˜¾ç¤ºä¸ºç©ºä½†nvidia-smiæ˜¾ç¤ºå ç”¨çš„æƒ…å†µã€‚
"""

import subprocess
import os
import logging
import glob
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_kernel_modules():
    """æ£€æŸ¥åŠ è½½çš„å†…æ ¸æ¨¡å—"""
    try:
        result = subprocess.run(['lsmod'], capture_output=True, text=True)
        modules = []

        for line in result.stdout.split('\n'):
            if 'nvidia' in line.lower() or 'cuda' in line.lower():
                modules.append(line.strip())

        logger.info("ç›¸å…³å†…æ ¸æ¨¡å—:")
        for module in modules:
            logger.info(f"  {module}")

        return modules

    except Exception as e:
        logger.error(f"æ£€æŸ¥å†…æ ¸æ¨¡å—æ—¶å‡ºé”™: {e}")
        return []

def check_tensorrt_engines():
    """æ£€æŸ¥TensorRTå¼•æ“æ–‡ä»¶"""
    engine_paths = [
        "engines/",
        "/tmp/engines/",
        "/var/tmp/engines/",
        "~/.cache/torch/engines/",
        "~/.cache/tensorrt/",
        "./cache/",
        ".torch/"
    ]

    total_engines = 0
    total_size = 0

    logger.info("æ£€æŸ¥TensorRTå¼•æ“æ–‡ä»¶...")

    for path in engine_paths:
        expanded_path = os.path.expanduser(path)
        if os.path.exists(expanded_path):
            logger.info(f"æ£€æŸ¥ç›®å½•: {expanded_path}")

            # æŸ¥æ‰¾.engineæ–‡ä»¶
            engine_files = []
            for pattern in ["*.engine", "**/*.engine"]:
                engine_files.extend(glob.glob(os.path.join(expanded_path, pattern), recursive=True))

            if engine_files:
                logger.info(f"  å‘ç° {len(engine_files)} ä¸ªå¼•æ“æ–‡ä»¶:")
                for engine_file in engine_files:
                    try:
                        file_size = os.path.getsize(engine_file)
                        total_size += file_size
                        total_engines += 1
                        logger.info(f"    {engine_file} ({file_size / 1024**2:.1f}MB)")
                    except Exception as e:
                        logger.warning(f"    æ— æ³•è¯»å– {engine_file}: {e}")
            else:
                logger.info(f"  æœªå‘ç°å¼•æ“æ–‡ä»¶")
        else:
            logger.info(f"  ç›®å½•ä¸å­˜åœ¨: {expanded_path}")

    logger.info(f"æ€»å…±å‘ç° {total_engines} ä¸ªTensorRTå¼•æ“æ–‡ä»¶ï¼Œæ€»å¤§å° {total_size / 1024**2:.1f}MB")
    return total_engines, total_size

def check_cuda_contexts():
    """æ£€æŸ¥CUDAä¸Šä¸‹æ–‡"""
    try:
        # å°è¯•å¯¼å…¥pycudaæ¥æ£€æŸ¥CUDAä¸Šä¸‹æ–‡
        try:
            import pycuda.driver as cuda
            import pycuda.tools

            cuda.init()
            device_count = cuda.Device.count()

            logger.info(f"CUDAè®¾å¤‡æ•°é‡: {device_count}")

            for device_id in range(device_count):
                device = cuda.Device(device_id)
                context = device.make_context()

                try:
                    # è·å–å†…å­˜ä¿¡æ¯
                    free_mem, total_mem = context.get_memory()
                    used_mem = total_mem - free_mem

                    logger.info(f"è®¾å¤‡ {device_id} ({device.name()}):")
                    logger.info(f"  æ€»å†…å­˜: {total_mem / 1024**2:.1f}MB")
                    logger.info(f"  å·²ä½¿ç”¨: {used_mem / 1024**2:.1f}MB")
                    logger.info(f"  å¯ç”¨: {free_mem / 1024**2:.1f}MB")

                    # æ£€æŸ¥ä¸Šä¸‹æ–‡ä¿¡æ¯
                    logger.info(f"  ä¸Šä¸‹æ–‡ä¿¡æ¯:")
                    logger.info(f"    APIç‰ˆæœ¬: {context.get_api_version()}")

                finally:
                    context.pop()
                    context.detach()

            return True

        except ImportError:
            logger.warning("pycudaæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥CUDAä¸Šä¸‹æ–‡")
            return False

    except Exception as e:
        logger.error(f"æ£€æŸ¥CUDAä¸Šä¸‹æ–‡æ—¶å‡ºé”™: {e}")
        return False

def check_process_handles():
    """æ£€æŸ¥å¯èƒ½æŒæœ‰GPUå¥æŸ„çš„è¿›ç¨‹"""
    try:
        # ä½¿ç”¨lsofæ£€æŸ¥è®¾å¤‡æ–‡ä»¶
        device_files = [
            "/dev/nvidia0",
            "/dev/nvidiactl",
            "/dev/nvidia-uvm",
            "/dev/nvidia-caps"
        ]

        for device_file in device_files:
            if os.path.exists(device_file):
                try:
                    result = subprocess.run(['lsof', device_file], capture_output=True, text=True)
                    if result.stdout.strip():
                        logger.info(f"è®¾å¤‡æ–‡ä»¶ {device_file} è¢«ä»¥ä¸‹è¿›ç¨‹ä½¿ç”¨:")
                        for line in result.stdout.strip().split('\n'):
                            if line.strip():
                                logger.info(f"  {line}")
                    else:
                        logger.info(f"è®¾å¤‡æ–‡ä»¶ {device_file} æœªè¢«ä½¿ç”¨")
                except FileNotFoundError:
                    logger.warning(f"lsofå‘½ä»¤æœªæ‰¾åˆ°ï¼Œæ— æ³•æ£€æŸ¥ {device_file}")
                except Exception as e:
                    logger.warning(f"æ£€æŸ¥ {device_file} æ—¶å‡ºé”™: {e}")

    except Exception as e:
        logger.error(f"æ£€æŸ¥è¿›ç¨‹å¥æŸ„æ—¶å‡ºé”™: {e}")

def check_nvidia_persistence_mode():
    """æ£€æŸ¥NVIDIAæŒä¹…åŒ–æ¨¡å¼"""
    try:
        result = subprocess.run(['nvidia-smi', '-q', '-d', 'PERSISTENCE_MODE'], capture_output=True, text=True)
        if result.returncode == 0:
            for line in result.stdout.split('\n'):
                if 'Persistence Mode' in line:
                    logger.info(f"æŒä¹…åŒ–æ¨¡å¼: {line.strip()}")
        else:
            logger.warning("æ— æ³•æŸ¥è¯¢æŒä¹…åŒ–æ¨¡å¼")
    except Exception as e:
        logger.error(f"æ£€æŸ¥æŒä¹…åŒ–æ¨¡å¼æ—¶å‡ºé”™: {e}")

def check_driver_version():
    """æ£€æŸ¥é©±åŠ¨ç‰ˆæœ¬ä¿¡æ¯"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=driver_version', '--format=csv,noheader,nounits'], capture_output=True, text=True)
        if result.returncode == 0:
            versions = result.stdout.strip().split('\n')
            for i, version in enumerate(versions):
                if version.strip():
                    logger.info(f"GPU {i} é©±åŠ¨ç‰ˆæœ¬: {version.strip()}")
    except Exception as e:
        logger.error(f"æ£€æŸ¥é©±åŠ¨ç‰ˆæœ¬æ—¶å‡ºé”™: {e}")

def diagnose_memory_leak():
    """è¯Šæ–­å†…å­˜æ³„æ¼åŸå› """
    logger.info("å¼€å§‹GPUå†…å­˜è¯Šæ–­...")

    # 1. æ£€æŸ¥é©±åŠ¨ç‰ˆæœ¬
    logger.info("=== é©±åŠ¨ç‰ˆæœ¬ä¿¡æ¯ ===")
    check_driver_version()

    # 2. æ£€æŸ¥æŒä¹…åŒ–æ¨¡å¼
    logger.info("\n=== æŒä¹…åŒ–æ¨¡å¼ ===")
    check_nvidia_persistence_mode()

    # 3. æ£€æŸ¥å†…æ ¸æ¨¡å—
    logger.info("\n=== å†…æ ¸æ¨¡å— ===")
    check_kernel_modules()

    # 4. æ£€æŸ¥TensorRTå¼•æ“
    logger.info("\n=== TensorRTå¼•æ“ ===")
    total_engines, total_size = check_tensorrt_engines()

    # 5. æ£€æŸ¥CUDAä¸Šä¸‹æ–‡
    logger.info("\n=== CUDAä¸Šä¸‹æ–‡ ===")
    has_cuda_contexts = check_cuda_contexts()

    # 6. æ£€æŸ¥è¿›ç¨‹å¥æŸ„
    logger.info("\n=== è¿›ç¨‹å¥æŸ„ ===")
    check_process_handles()

    # 7. åˆ†æå’Œå»ºè®®
    logger.info("\n=== è¯Šæ–­ç»“æœå’Œå»ºè®® ===")

    if total_engines > 0:
        logger.info(f"âš ï¸  å‘ç° {total_engines} ä¸ªTensorRTå¼•æ“æ–‡ä»¶ ({total_size / 1024**2:.1f}MB)")
        logger.info("ğŸ’¡ å»ºè®®:")
        logger.info("   1. TensorRTå¼•æ“å¯èƒ½æŒç»­å ç”¨GPUå†…å­˜")
        logger.info("   2. è¿è¡Œ: python scripts/force_cleanup.py")
        logger.info("   3. æˆ–è€…æ‰‹åŠ¨åˆ é™¤å¼•æ“æ–‡ä»¶")

    if not has_cuda_contexts:
        logger.info("âœ… æ— æ´»è·ƒçš„CUDAä¸Šä¸‹æ–‡")
    else:
        logger.info("âš ï¸  å­˜åœ¨æ´»è·ƒçš„CUDAä¸Šä¸‹æ–‡")

    # è¿è¡Œæœ€ç»ˆçš„nvidia-smiæ£€æŸ¥
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            logger.info("\n=== å½“å‰nvidia-smiçŠ¶æ€ ===")
            # æå–å…³é”®ä¿¡æ¯
            lines = result.stdout.split('\n')
            for line in lines:
                if 'Memory-Usage' in line or 'python' in line or 'MiB' in line:
                    logger.info(f"  {line}")

                # æŸ¥æ‰¾GPUè¿›ç¨‹
                if 'Processes:' in line:
                    # è·å–æ¥ä¸‹æ¥çš„å‡ è¡Œ
                    process_lines = []
                    for i in range(lines.index(line) + 1, len(lines)):
                        if lines[i].strip():
                            process_lines.append(lines[i].strip())
                        else:
                            break

                    if process_lines:
                        logger.info("  GPUè¿›ç¨‹:")
                        for process_line in process_lines:
                            logger.info(f"    {process_line}")
                    else:
                        logger.info("  æ— GPUè¿›ç¨‹æ˜¾ç¤º")
                        logger.info("  âš ï¸  è¿™å¯èƒ½æ˜¯TensorRTå¼•æ“æˆ–å…¶ä»–é©±åŠ¨çº§åˆ«çš„å†…å­˜å ç”¨")

    except Exception as e:
        logger.error(f"æœ€ç»ˆnvidia-smiæ£€æŸ¥å¤±è´¥: {e}")

    logger.info("\nè¯Šæ–­å®Œæˆ!")

if __name__ == "__main__":
    diagnose_memory_leak()