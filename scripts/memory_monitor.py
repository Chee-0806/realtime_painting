#!/usr/bin/env python3
"""
å†…å­˜ç›‘æ§è„šæœ¬

ç”¨äºå®æ—¶ç›‘æ§GPUå’ŒCPUå†…å­˜ä½¿ç”¨æƒ…å†µã€‚
"""

import psutil
import torch
import time
import subprocess
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_gpu_memory_info():
    """è·å–GPUå†…å­˜ä¿¡æ¯"""
    if not torch.cuda.is_available():
        return {"available": False}

    try:
        device_count = torch.cuda.device_count()
        gpu_info = {"available": True, "devices": []}

        for device_id in range(device_count):
            allocated = torch.cuda.memory_allocated(device_id)
            reserved = torch.cuda.memory_reserved(device_id)
            total = torch.cuda.get_device_properties(device_id).total_memory

            device_info = {
                "device_id": device_id,
                "name": torch.cuda.get_device_name(device_id),
                "total_gb": total / 1024**3,
                "allocated_gb": allocated / 1024**3,
                "reserved_gb": reserved / 1024**3,
                "free_gb": (total - allocated) / 1024**3,
                "utilization_percent": (allocated / total) * 100
            }

            gpu_info["devices"].append(device_info)

        return gpu_info

    except Exception as e:
        logger.error(f"è·å–GPUä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return {"available": False, "error": str(e)}

def get_nvidia_smi_info():
    """è·å–nvidia-smiä¿¡æ¯"""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=index,name,memory.total,memory.used,memory.free,utilization.gpu,temperature.gpu,power.draw', '--format=csv,noheader,nounits'],
            capture_output=True, text=True
        )

        if result.returncode == 0:
            nvidia_info = []
            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    parts = [p.strip() for p in line.split(',')]
                    if len(parts) >= 8:
                        nvidia_info.append({
                            "index": int(parts[0]),
                            "name": parts[1],
                            "memory_total_mb": int(parts[2]),
                            "memory_used_mb": int(parts[3]),
                            "memory_free_mb": int(parts[4]),
                            "utilization_percent": int(parts[5]),
                            "temperature_c": int(parts[6]),
                            "power_watts": float(parts[7])
                        })
            return nvidia_info
        else:
            return []

    except Exception as e:
        logger.error(f"è·å–nvidia-smiä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return []

def get_cpu_memory_info():
    """è·å–CPUå†…å­˜ä¿¡æ¯"""
    try:
        memory = psutil.virtual_memory()
        current_process = psutil.Process()

        # ç³»ç»Ÿå†…å­˜
        system_info = {
            "total_gb": memory.total / 1024**3,
            "available_gb": memory.available / 1024**3,
            "used_gb": memory.used / 1024**3,
            "percent": memory.percent
        }

        # å½“å‰è¿›ç¨‹å†…å­˜
        process_memory = current_process.memory_info()
        process_info = {
            "rss_gb": process_memory.rss / 1024**3,  # ç‰©ç†å†…å­˜
            "vms_gb": process_memory.vms / 1024**3,  # è™šæ‹Ÿå†…å­˜
            "percent": current_process.memory_percent()
        }

        return {
            "system": system_info,
            "current_process": process_info
        }

    except Exception as e:
        logger.error(f"è·å–CPUå†…å­˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return {}

def find_python_processes():
    """æŸ¥æ‰¾Pythonè¿›ç¨‹"""
    python_processes = []

    for proc in psutil.process_iter(['pid', 'name', 'cmdline', 'memory_info', 'cpu_percent']):
        try:
            if proc.info['name'] == 'python':
                cmdline = proc.info['cmdline']
                memory_info = proc.info['memory_info']

                process_info = {
                    "pid": proc.info['pid'],
                    "cmdline": ' '.join(cmdline) if cmdline else '',
                    "rss_gb": memory_info.rss / 1024**3 if memory_info else 0,
                    "vms_gb": memory_info.vms / 1024**3 if memory_info else 0,
                    "cpu_percent": proc.info['cpu_percent'] if proc.info['cpu_percent'] else 0
                }

                # æ ‡è®°ç›¸å…³è¿›ç¨‹
                if cmdline and any(keyword in ' '.join(cmdline) for keyword in [
                    'streamdiffusion', 'realtime_painting', 'uvicorn', 'fastapi'
                ]):
                    process_info["relevant"] = True
                else:
                    process_info["relevant"] = False

                python_processes.append(process_info)

        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue

    return python_processes

def format_memory_info():
    """æ ¼å¼åŒ–å†…å­˜ä¿¡æ¯"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # GPUä¿¡æ¯
    gpu_info = get_gpu_memory_info()
    nvidia_info = get_nvidia_smi_info()

    # CPUä¿¡æ¯
    cpu_info = get_cpu_memory_info()
    python_processes = find_python_processes()

    # æ‰“å°æŠ¥å‘Š
    print(f"\n{'='*80}")
    print(f"å†…å­˜ç›‘æ§æŠ¥å‘Š - {timestamp}")
    print(f"{'='*80}")

    # GPUéƒ¨åˆ†
    print(f"\nğŸ® GPU å†…å­˜çŠ¶æ€:")
    if gpu_info.get("available"):
        for device in gpu_info["devices"]:
            print(f"  è®¾å¤‡ {device['device_id']}: {device['name']}")
            print(f"    æ€»å†…å­˜: {device['total_gb']:.2f}GB")
            print(f"    å·²åˆ†é…: {device['allocated_gb']:.2f}GB ({device['utilization_percent']:.1f}%)")
            print(f"    å·²ä¿ç•™: {device['reserved_gb']:.2f}GB")
            print(f"    å¯ç”¨: {device['free_gb']:.2f}GB")

    # nvidia-smiä¿¡æ¯
    if nvidia_info:
        print(f"\nğŸ“Š nvidia-smi è¯¦ç»†ä¿¡æ¯:")
        for gpu in nvidia_info:
            print(f"  GPU {gpu['index']}: {gpu['name']}")
            print(f"    å†…å­˜: {gpu['memory_used_mb']}/{gpu['memory_total_mb']}MB ({gpu['memory_used_mb']/gpu['memory_total_mb']*100:.1f}%)")
            print(f"    åˆ©ç”¨ç‡: {gpu['utilization_percent']}%")
            print(f"    æ¸©åº¦: {gpu['temperature_c']}Â°C")
            print(f"    åŠŸè€—: {gpu['power_watts']:.1f}W")

    # CPUéƒ¨åˆ†
    print(f"\nğŸ’» CPU å†…å­˜çŠ¶æ€:")
    if cpu_info.get("system"):
        system = cpu_info["system"]
        print(f"  ç³»ç»Ÿæ€»å†…å­˜: {system['total_gb']:.2f}GB")
        print(f"  å·²ä½¿ç”¨: {system['used_gb']:.2f}GB ({system['percent']:.1f}%)")
        print(f"  å¯ç”¨: {system['available_gb']:.2f}GB")

    if cpu_info.get("current_process"):
        process = cpu_info["current_process"]
        print(f"  å½“å‰è¿›ç¨‹å†…å­˜:")
        print(f"    ç‰©ç†å†…å­˜(RSS): {process['rss_gb']:.2f}GB ({process['percent']:.1f}%)")
        print(f"    è™šæ‹Ÿå†…å­˜(VMS): {process['vms_gb']:.2f}GB")

    # Pythonè¿›ç¨‹
    relevant_processes = [p for p in python_processes if p.get("relevant")]
    other_processes = [p for p in python_processes if not p.get("relevant")]

    print(f"\nğŸ Python è¿›ç¨‹:")
    if relevant_processes:
        print(f"  ç›¸å…³è¿›ç¨‹ ({len(relevant_processes)}):")
        for proc in relevant_processes:
            print(f"    PID {proc['pid']}: {proc['rss_gb']:.2f}GB, {proc['cpu_percent']:.1f}% CPU")
            print(f"      {proc['cmdline'][:80]}...")

    if other_processes:
        print(f"  å…¶ä»–Pythonè¿›ç¨‹ ({len(other_processes)}):")
        for proc in other_processes[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"    PID {proc['pid']}: {proc['rss_gb']:.2f}GB")
        if len(other_processes) > 5:
            print(f"    ... è¿˜æœ‰ {len(other_processes) - 5} ä¸ªè¿›ç¨‹")

    # å†…å­˜ä½¿ç”¨å»ºè®®
    print(f"\nğŸ’¡ å†…å­˜ä½¿ç”¨åˆ†æ:")

    # GPUå†…å­˜åˆ†æ
    if gpu_info.get("available"):
        total_gpu_allocated = sum(device["allocated_gb"] for device in gpu_info["devices"])
        total_gpu_reserved = sum(device["reserved_gb"] for device in gpu_info["devices"])

        if total_gpu_allocated > 10:
            print(f"  âš ï¸  GPUå†…å­˜ä½¿ç”¨è¾ƒé«˜ ({total_gpu_allocated:.1f}GB)ï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜æ³„æ¼")
        elif total_gpu_allocated > 5:
            print(f"  ğŸ”¶ GPUå†…å­˜ä½¿ç”¨ä¸­ç­‰ ({total_gpu_allocated:.1f}GB)")
        else:
            print(f"  âœ… GPUå†…å­˜ä½¿ç”¨æ­£å¸¸ ({total_gpu_allocated:.1f}GB)")

        if total_gpu_reserved > total_gpu_allocated * 1.2:
            print(f"  âš ï¸  GPUä¿ç•™å†…å­˜è¾ƒå¤š ({total_gpu_reserved:.1f}GB)ï¼Œå»ºè®®è¿è¡Œ torch.cuda.empty_cache()")

    # CPUå†…å­˜åˆ†æ
    if cpu_info.get("system") and cpu_info["system"]["percent"] > 80:
        print(f"  âš ï¸  ç³»ç»ŸCPUå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ ({cpu_info['system']['percent']:.1f}%)")
    elif cpu_info.get("current_process") and cpu_info["current_process"]["rss_gb"] > 2:
        print(f"  âš ï¸  å½“å‰è¿›ç¨‹CPUå†…å­˜ä½¿ç”¨è¾ƒé«˜ ({cpu_info['current_process']['rss_gb']:.1f}GB)")

    # è¿›ç¨‹åˆ†æ
    total_python_memory = sum(p["rss_gb"] for p in relevant_processes)
    if total_python_memory > 8:
        print(f"  âš ï¸  Pythonç›¸å…³è¿›ç¨‹æ€»å†…å­˜ä½¿ç”¨è¾ƒé«˜ ({total_python_memory:.1f}GB)")

def monitor_continuous(interval_seconds=10, max_iterations=60):
    """æŒç»­ç›‘æ§"""
    print(f"å¼€å§‹æŒç»­ç›‘æ§ï¼Œæ¯ {interval_seconds} ç§’æ›´æ–°ä¸€æ¬¡ï¼Œæœ€å¤š {max_iterations} æ¬¡")
    print("æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")

    try:
        for i in range(max_iterations):
            format_memory_info()
            if i < max_iterations - 1:  # æœ€åä¸€æ¬¡ä¸éœ€è¦ç­‰å¾…
                time.sleep(interval_seconds)
    except KeyboardInterrupt:
        print("\nç›‘æ§å·²åœæ­¢")

def main():
    """ä¸»å‡½æ•°"""
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--continuous":
        interval = int(sys.argv[2]) if len(sys.argv) > 2 else 10
        max_iter = int(sys.argv[3]) if len(sys.argv) > 3 else 60
        monitor_continuous(interval, max_iter)
    else:
        format_memory_info()

if __name__ == "__main__":
    main()